# @package _global_

# to execute this experiment run:
# python run.py experiment=mnli


defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /module: text_classification.yaml
  - override /datamodule: nli-shots.yaml
  # - override /datamodule: nli-shots.yaml
  - override /callbacks: null
  - override /config_callbacks: trident.yaml
  - override /logger: wandb.yaml

seed: ${rnd:1,1000000}
kind: single
task: nli
shots: ???
test_after_training: false

trainer:
  max_epochs: 50
  devices: 1
  precision: "16-mixed"
  deterministic: true
  inference_mode: false
  num_sanity_val_steps: 0
  # gradient_clip_val: 1
  check_val_every_n_epoch: 10

module:
  _target_: src.modules.modeling.average.AverageTridentModule
  # avg_ckpts: null # "${callbacks.model_checkpoint_on_epoch.dirpath}"
  avg_ckpts: "${callbacks.model_checkpoint_on_epoch.dirpath}"
  compile: false
  scheduler: null
  # lora_cfg:
  #   task_type: "SEQ_CLS"
  #   inference_mode: false
  #   r: 8
  #   lora_alpha: 16
  #   lora_dropout: 0.1
  #   target_modules:
  #     - query
  #     - value

  model:
    # pretrained_model_name_or_path: "facebook/xlm-roberta-xl"
    pretrained_model_name_or_path: "roberta-large"
    # pretrained_model_name_or_path: "facebook/xlm-v-base"
    # pretrained_model_name_or_path: "microsoft/mdeberta-v3-base"
    num_labels: 3
    return_dict: true
  optimizer:
    lr: ???
logger:
  wandb:
    # name: "kind=${kind}_model=${rm_upto_backslash:${module.model.pretrained_model_name_or_path}}_shots=${shots}_lr=${module.optimizer.lr}_epochs=${trainer.max_epochs}_bs=${datamodule.dataloader_cfg.train.batch_size}_scheduler=${module.scheduler.num_warmup_steps}_seed=${seed}"
    name: "kind=${kind}_model=${rm_upto_backslash:${module.model.pretrained_model_name_or_path}}_shots=${shots}_lr=${module.optimizer.lr}_epochs=${trainer.max_epochs}_bs=${datamodule.dataloader_cfg.train.batch_size}_seed=${seed}"
    project: "ofa-${task}-shots"
    tags:
      - "${kind}"
      - "${rm_upto_backslash:${module.model.pretrained_model_name_or_path}}"
      - "bs=${datamodule.dataloader_cfg.train.batch_size}"
      - "lr=${module.optimizer.lr}"
      # - "scheduler=${module.scheduler.num_warmup_steps}"
      - "shots=${shots}"

callbacks:
  model_checkpoint_on_epoch:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: null # name of the logged metric which determines when model is improving
    # monitor: "validation_xnli_en/val/acc" # name of the logged metric which determines when model is improving
    # mode: "max" # name of the logged metric which determines when model is improving
    every_n_epochs: 25 # truncated length of NLI train / 16
    verbose: false
    save_top_k: -1 # -1 -> all models are saved
    filename: "epoch={epoch}_val={validation_xnli_en/val/acc:.4f}"
    save_last: false # additionaly always save model from last epoch
    dirpath: "${hydra:runtime.output_dir}/checkpoints/"
    save_weights_only: true
    auto_insert_metric_name: false
